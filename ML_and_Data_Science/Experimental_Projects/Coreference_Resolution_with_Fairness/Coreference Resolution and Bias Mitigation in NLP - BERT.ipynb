{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f30e14e5",
   "metadata": {},
   "source": [
    "# Coreference Resolution and Bias Mitigation in NLP\n",
    "\n",
    "## Project Overview\n",
    "Coreference resolution is the task of identifying when different words or phrases in a text refer to the same entity. This task is especially crucial in understanding text coherence, which helps in a variety of natural language processing (NLP) applications such as machine translation, summarization, sentiment analysis, and question answering. However, coreference resolution models often exhibit gender bias, associating certain roles or attributes more with one gender over another. This project explores ...\n",
    "\n",
    "In this project, I will use a transformer-based model to perform coreference resolution on the **Gendered Ambiguous Pronoun (GAP) dataset**. The project focuses on identifying and addressing gender bias within coreference resolution tasks.\n",
    "\n",
    "## Dataset Background\n",
    "The GAP dataset, developed by Google AI Language, is a gender-balanced corpus designed to help address gender bias in coreference resolution. The dataset contains examples where gendered pronouns (e.g., \"he\", \"she\") refer ambiguously to potential antecedents within the text. It provides annotated pairs of pronouns and candidate names, which the model needs to link correctly. This dataset is widely used for studying and reducing gender bias in coreference tasks.\n",
    "\n",
    "The GAP dataset is available on [Kaggle](https://www.kaggle.com/c/gendered-pronoun-resolution) and Google Research’s [GitHub repository](https://github.com/google-research-datasets/gap-coreference). Previous work on this dataset has focused on:\n",
    "\n",
    "- Fine-tuning models like BERT to achieve accurate coreference resolution.\n",
    "- Developing metrics to evaluate model fairness across genders.\n",
    "- Applying debiasing techniques to reduce model bias in gender-specific predictions.\n",
    "\n",
    "## Project Purpose\n",
    "In this project, I will first work with English-language coreference resolution to build a foundational understanding of bias in models. This initial approach will help prepare for a more complex challenge in my native language, Spanish. In Spanish, nouns and adjectives are gendered, which introduces additional nuances to coreference resolution. This complexity makes Spanish coreference resolution inherently more challenging, and ensuring fairness becomes even more critical.\n",
    "\n",
    "### Project Steps\n",
    "1. **Data Preparation and Exploration**: Load and explore the GAP dataset to understand its structure.\n",
    "2. **Baseline Model Building and Evaluation**: Train a baseline transformer model for coreference resolution.\n",
    "3. **Fairness Evaluation**: Quantify bias in the baseline model across genders.\n",
    "4. **Bias Mitigation Techniques**: Apply debiasing methods to improve fairness in coreference resolution.\n",
    "5. **Post-Debiasing Evaluation**: Re-evaluate the model to assess improvements in fairness.\n",
    "\n",
    "Let's begin by loading and exploring the dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea16d7fd",
   "metadata": {},
   "source": [
    "## Step 1: Importing Libraries and Loading the GAP Dataset\n",
    "\n",
    "I will start by importing the necessary libraries and loading the GAP dataset for coreference resolution. The dataset provides examples where pronouns ambiguously refer to two possible entities, allowing us to train a model to correctly resolve these references in a fair and unbiased manner.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b982853c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 22:02:45.557304: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-04 22:02:45.635697: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-11-04 22:02:45.954792: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-04 22:02:47.175222: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>development-1</td>\n",
       "      <td>Zoe Telford -- played the police officer girlf...</td>\n",
       "      <td>her</td>\n",
       "      <td>274</td>\n",
       "      <td>Cheryl Cassidy</td>\n",
       "      <td>191</td>\n",
       "      <td>True</td>\n",
       "      <td>Pauline</td>\n",
       "      <td>207</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/List_of_Teachers_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>development-2</td>\n",
       "      <td>He grew up in Evanston, Illinois the second ol...</td>\n",
       "      <td>His</td>\n",
       "      <td>284</td>\n",
       "      <td>MacKenzie</td>\n",
       "      <td>228</td>\n",
       "      <td>True</td>\n",
       "      <td>Bernard Leach</td>\n",
       "      <td>251</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Warren_MacKenzie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>development-3</td>\n",
       "      <td>He had been reelected to Congress, but resigne...</td>\n",
       "      <td>his</td>\n",
       "      <td>265</td>\n",
       "      <td>Angeloz</td>\n",
       "      <td>173</td>\n",
       "      <td>False</td>\n",
       "      <td>De la Sota</td>\n",
       "      <td>246</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>development-4</td>\n",
       "      <td>The current members of Crime have also perform...</td>\n",
       "      <td>his</td>\n",
       "      <td>321</td>\n",
       "      <td>Hell</td>\n",
       "      <td>174</td>\n",
       "      <td>False</td>\n",
       "      <td>Henry Rosenthal</td>\n",
       "      <td>336</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Crime_(band)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>development-5</td>\n",
       "      <td>Her Santa Fe Opera debut in 2005 was as Nuria ...</td>\n",
       "      <td>She</td>\n",
       "      <td>437</td>\n",
       "      <td>Kitty Oppenheimer</td>\n",
       "      <td>219</td>\n",
       "      <td>False</td>\n",
       "      <td>Rivera</td>\n",
       "      <td>294</td>\n",
       "      <td>True</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Jessica_Rivera</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                                               Text Pronoun  \\\n",
       "0  development-1  Zoe Telford -- played the police officer girlf...     her   \n",
       "1  development-2  He grew up in Evanston, Illinois the second ol...     His   \n",
       "2  development-3  He had been reelected to Congress, but resigne...     his   \n",
       "3  development-4  The current members of Crime have also perform...     his   \n",
       "4  development-5  Her Santa Fe Opera debut in 2005 was as Nuria ...     She   \n",
       "\n",
       "   Pronoun-offset                  A  A-offset  A-coref                B  \\\n",
       "0             274     Cheryl Cassidy       191     True          Pauline   \n",
       "1             284          MacKenzie       228     True    Bernard Leach   \n",
       "2             265            Angeloz       173    False       De la Sota   \n",
       "3             321               Hell       174    False  Henry Rosenthal   \n",
       "4             437  Kitty Oppenheimer       219    False           Rivera   \n",
       "\n",
       "   B-offset  B-coref                                                URL  \n",
       "0       207    False  http://en.wikipedia.org/wiki/List_of_Teachers_...  \n",
       "1       251    False      http://en.wikipedia.org/wiki/Warren_MacKenzie  \n",
       "2       246     True  http://en.wikipedia.org/wiki/Jos%C3%A9_Manuel_...  \n",
       "3       336     True          http://en.wikipedia.org/wiki/Crime_(band)  \n",
       "4       294     True        http://en.wikipedia.org/wiki/Jessica_Rivera  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing essential libraries\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the GAP dataset (replace with local path if necessary)\n",
    "url = \"https://raw.githubusercontent.com/google-research-datasets/gap-coreference/master/gap-development.tsv\"\n",
    "data = pd.read_csv(url, delimiter='\\t')\n",
    "\n",
    "# Display first few rows\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0d9646",
   "metadata": {},
   "source": [
    "## Dataset Structure\n",
    "\n",
    "The GAP dataset contains the following columns:\n",
    "- **Text**: The passage containing an ambiguous pronoun.\n",
    "- **Pronoun**: The pronoun in question.\n",
    "- **A** and **B**: The two possible entities that the pronoun may refer to.\n",
    "- **A-coref** and **B-coref**: Labels indicating if the pronoun refers to A or B.\n",
    "- **Gender**: Gender of the pronoun (e.g., \"he\", \"she\").\n",
    "\n",
    "These columns provide the context needed to train a model for pronoun disambiguation and coreference resolution.\n",
    "\n",
    "I will preprocess the dataset by tokenizing the text, encoding the pronoun and candidate pairs, and splitting the data into training and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5829b44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000 entries, 0 to 1999\n",
      "Data columns (total 11 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   ID              2000 non-null   object\n",
      " 1   Text            2000 non-null   object\n",
      " 2   Pronoun         2000 non-null   object\n",
      " 3   Pronoun-offset  2000 non-null   int64 \n",
      " 4   A               2000 non-null   object\n",
      " 5   A-offset        2000 non-null   int64 \n",
      " 6   A-coref         2000 non-null   bool  \n",
      " 7   B               2000 non-null   object\n",
      " 8   B-offset        2000 non-null   int64 \n",
      " 9   B-coref         2000 non-null   bool  \n",
      " 10  URL             2000 non-null   object\n",
      "dtypes: bool(2), int64(3), object(6)\n",
      "memory usage: 144.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f037c3c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>Pronoun-offset</th>\n",
       "      <th>A</th>\n",
       "      <th>A-offset</th>\n",
       "      <th>A-coref</th>\n",
       "      <th>B</th>\n",
       "      <th>B-offset</th>\n",
       "      <th>B-coref</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000.00000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2000</td>\n",
       "      <td>1999</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1793</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1774</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>development-1</td>\n",
       "      <td>According to her mother, Tatyana Vladimovna, D...</td>\n",
       "      <td>her</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Elizabeth</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>Mary</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>http://en.wikipedia.org/wiki/Wilhelmina_Slater</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>534</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1126</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1075</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>324.963500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239.77800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.535500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.788591</td>\n",
       "      <td>NaN</td>\n",
       "      <td>111.15768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>113.226357</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>274.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.75000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>237.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>316.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>239.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>294.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>301.25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>358.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1135.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>971.00000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1098.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID                                               Text  \\\n",
       "count            2000                                               2000   \n",
       "unique           2000                                               1999   \n",
       "top     development-1  According to her mother, Tatyana Vladimovna, D...   \n",
       "freq                1                                                  2   \n",
       "mean              NaN                                                NaN   \n",
       "std               NaN                                                NaN   \n",
       "min               NaN                                                NaN   \n",
       "25%               NaN                                                NaN   \n",
       "50%               NaN                                                NaN   \n",
       "75%               NaN                                                NaN   \n",
       "max               NaN                                                NaN   \n",
       "\n",
       "       Pronoun  Pronoun-offset          A    A-offset A-coref     B  \\\n",
       "count     2000     2000.000000       2000  2000.00000    2000  2000   \n",
       "unique       9             NaN       1793         NaN       2  1774   \n",
       "top        her             NaN  Elizabeth         NaN   False  Mary   \n",
       "freq       534             NaN          7         NaN    1126     8   \n",
       "mean       NaN      324.963500        NaN   239.77800     NaN   NaN   \n",
       "std        NaN       98.788591        NaN   111.15768     NaN   NaN   \n",
       "min        NaN        3.000000        NaN     0.00000     NaN   NaN   \n",
       "25%        NaN      274.000000        NaN   179.75000     NaN   NaN   \n",
       "50%        NaN      316.000000        NaN   239.00000     NaN   NaN   \n",
       "75%        NaN      370.000000        NaN   301.25000     NaN   NaN   \n",
       "max        NaN     1135.000000        NaN   971.00000     NaN   NaN   \n",
       "\n",
       "           B-offset B-coref                                             URL  \n",
       "count   2000.000000    2000                                            2000  \n",
       "unique          NaN       2                                            1834  \n",
       "top             NaN   False  http://en.wikipedia.org/wiki/Wilhelmina_Slater  \n",
       "freq            NaN    1075                                               4  \n",
       "mean     300.535500     NaN                                             NaN  \n",
       "std      113.226357     NaN                                             NaN  \n",
       "min       16.000000     NaN                                             NaN  \n",
       "25%      237.000000     NaN                                             NaN  \n",
       "50%      294.000000     NaN                                             NaN  \n",
       "75%      358.000000     NaN                                             NaN  \n",
       "max     1098.000000     NaN                                             NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c7d21",
   "metadata": {},
   "source": [
    "Identify if any rows have missing values, especially in essential columns like Text, Pronoun, A, and B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ff885c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                0\n",
       "Text              0\n",
       "Pronoun           0\n",
       "Pronoun-offset    0\n",
       "A                 0\n",
       "A-offset          0\n",
       "A-coref           0\n",
       "B                 0\n",
       "B-offset          0\n",
       "B-coref           0\n",
       "URL               0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e44e5",
   "metadata": {},
   "source": [
    "Check the balance of classes for A-coref and B-coref labels to see if the dataset is balanced between pronouns that refer to A vs. B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fa3d00d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B-coref\n",
       "False    0.5375\n",
       "True     0.4625\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['A-coref'].value_counts(normalize=True)  # Distribution of labels for `A`\n",
    "data['B-coref'].value_counts(normalize=True)  # Distribution of labels for `B`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92597a9",
   "metadata": {},
   "source": [
    "Determine the distribution of male vs. female pronouns to see if the dataset is gender-balanced, as this could impact model fairness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b7793f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pronoun\n",
       "her    0.2670\n",
       "his    0.2480\n",
       "she    0.1245\n",
       "he     0.1175\n",
       "She    0.0895\n",
       "He     0.0690\n",
       "him    0.0490\n",
       "Her    0.0190\n",
       "His    0.0165\n",
       "Name: proportion, dtype: float64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Pronoun'].value_counts(normalize=True)  # Proportion of each pronoun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274b5a25",
   "metadata": {},
   "source": [
    " Calculate the average and range of text lengths to see if any tokenization adjustments (e.g., truncation or max length) are needed for BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4acd357c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2000.00000\n",
       "mean       71.20350\n",
       "std        20.52705\n",
       "min        16.00000\n",
       "25%        58.00000\n",
       "50%        68.00000\n",
       "75%        82.00000\n",
       "max       204.00000\n",
       "Name: Text_Length, dtype: float64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Text_Length'] = data['Text'].apply(lambda x: len(x.split()))  # Word count\n",
    "data['Text_Length'].describe()  # Summary of text length statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfa5a81",
   "metadata": {},
   "source": [
    "Check if entities A and B overlap in the text. Analyzing how frequently the entities appear close together can give insights into the difficulty of the coreference resolution task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c4b92133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity A in Text: A_in_Text\n",
      "True    1.0\n",
      "Name: proportion, dtype: float64\n",
      "Entity B in Text: B_in_Text\n",
      "True    1.0\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Count occurrences of `A` and `B` within the text\n",
    "data['A_in_Text'] = data.apply(lambda x: x['A'] in x['Text'], axis=1)\n",
    "data['B_in_Text'] = data.apply(lambda x: x['B'] in x['Text'], axis=1)\n",
    "print(\"Entity A in Text:\", data['A_in_Text'].value_counts(normalize=True))\n",
    "print(\"Entity B in Text:\", data['B_in_Text'].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b2497a",
   "metadata": {},
   "source": [
    "Find the most frequently occurring entities and pronouns in the dataset, as this could reveal any potential biases or imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "97a535ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common entities A: A\n",
      "Elizabeth    7\n",
      "Ellen        6\n",
      "Jones        5\n",
      "Maria        5\n",
      "Margaret     5\n",
      "Helen        5\n",
      "Anne         5\n",
      "Thomas       4\n",
      "Alice        4\n",
      "James        4\n",
      "Name: count, dtype: int64\n",
      "Most common entities B: B\n",
      "Mary         8\n",
      "Smith        5\n",
      "Emily        5\n",
      "Jackson      5\n",
      "Alice        5\n",
      "Margaret     4\n",
      "King         4\n",
      "Stephanie    4\n",
      "Isabel       4\n",
      "Daisy        4\n",
      "Name: count, dtype: int64\n",
      "Most common pronouns: Pronoun\n",
      "her    534\n",
      "his    496\n",
      "she    249\n",
      "he     235\n",
      "She    179\n",
      "He     138\n",
      "him     98\n",
      "Her     38\n",
      "His     33\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check most common entities and pronouns\n",
    "print(\"Most common entities A:\", data['A'].value_counts().head(10))\n",
    "print(\"Most common entities B:\", data['B'].value_counts().head(10))\n",
    "print(\"Most common pronouns:\", data['Pronoun'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b633e132",
   "metadata": {},
   "source": [
    "Understand the context in which ambiguous pronouns appear, which is essential for designing models that can resolve these ambiguities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60fd10bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Pronoun</th>\n",
       "      <th>B</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>760</th>\n",
       "      <td>Ten seasons after they return to the Abbey, we...</td>\n",
       "      <td>her</td>\n",
       "      <td>Martha</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>761</th>\n",
       "      <td>This acquisition secured communication with He...</td>\n",
       "      <td>his</td>\n",
       "      <td>Pedro</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1827</th>\n",
       "      <td>Irene married her second husband, Harold E. Kn...</td>\n",
       "      <td>her</td>\n",
       "      <td>Ryan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>Rosie planned to move away with Darren, Demi a...</td>\n",
       "      <td>her</td>\n",
       "      <td>Demi</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1157</th>\n",
       "      <td>Because of the close friendship between Marcie...</td>\n",
       "      <td>she</td>\n",
       "      <td>Marcie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   Text Pronoun       B\n",
       "760   Ten seasons after they return to the Abbey, we...     her  Martha\n",
       "761   This acquisition secured communication with He...     his   Pedro\n",
       "1827  Irene married her second husband, Harold E. Kn...     her    Ryan\n",
       "1040  Rosie planned to move away with Darren, Demi a...     her    Demi\n",
       "1157  Because of the close friendship between Marcie...     she  Marcie"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample a few rows where A-coref or B-coref is True to see text context\n",
    "data[data['A-coref'] == 1][['Text', 'Pronoun', 'A']].sample(5)\n",
    "data[data['B-coref'] == 1][['Text', 'Pronoun', 'B']].sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef485a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8897a3e3",
   "metadata": {},
   "source": [
    "Check if there’s any correlation between the length of the text and whether the pronoun refers to A or B, which might provide insights into patterns that the model could exploit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf2e7fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A-coref\n",
       "False    72.080817\n",
       "True     70.073227\n",
       "Name: Text_Length, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('A-coref')['Text_Length'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5f08423b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B-coref\n",
       "False    70.835349\n",
       "True     71.631351\n",
       "Name: Text_Length, dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate average text length by coreference label\n",
    "\n",
    "data.groupby('B-coref')['Text_Length'].mean()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c019ce9d",
   "metadata": {},
   "source": [
    "## Step 2: Data Preprocessing\n",
    "\n",
    "To train a transformer-based model for coreference resolution, I'll preprocess the text data by tokenizing the input and encoding the labels for candidate pairs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db38bcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d8bfcbac89b4673bf3a2f9d480776a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f093783b712144c5a5544ce42ff32cf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b5b0c7ddb14cebaa10cf131a1be465",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c428117b0f4dabae66fe7d62be5373",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c3a7384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID                                                    development-1\n",
      "Text              Zoe Telford -- played the police officer girlf...\n",
      "Pronoun                                                         her\n",
      "Pronoun-offset                                                  274\n",
      "A                                                    Cheryl Cassidy\n",
      "A-offset                                                        191\n",
      "A-coref                                                        True\n",
      "B                                                           Pauline\n",
      "B-offset                                                        207\n",
      "B-coref                                                       False\n",
      "URL               http://en.wikipedia.org/wiki/List_of_Teachers_...\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb9b0c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example text, pronoun, and two entities\n",
    "example_text = data.loc[0, 'Text']\n",
    "example_pronoun = data.loc[0, 'Pronoun']\n",
    "entity_A = data.loc[0, 'A']\n",
    "entity_B = data.loc[0, 'B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5071e74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format the text with each entity as a possible referent\n",
    "text_with_A = f\"{example_text} [SEP] Pronoun: {example_pronoun} [SEP] Entity: {entity_A}\"\n",
    "text_with_B = f\"{example_text} [SEP] Pronoun: {example_pronoun} [SEP] Entity: {entity_B}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6f5e516",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 22:55:27.667027: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-11-04 22:55:27.668816: W tensorflow/core/common_runtime/gpu/gpu_device.cc:2251] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[  101, 11199, 10093,  3877,  1011,  1011,  2209,  1996,  2610,\n",
       "          2961,  6513,  1997,  4079,  1010,  8538,  1012, 14019,  2011,\n",
       "          4079,  1999,  1996,  2345,  2792,  1997,  2186,  1015,  1010,\n",
       "          2044,  2002,  7771,  2007,  8437,  1010,  1998,  2003,  2025,\n",
       "          2464,  2153,  1012, 18188,  2726,  2209, 19431, 13737,  1010,\n",
       "         15595,  1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340,\n",
       "         11136,  1999,  4079,  1005,  1055,  2465,  1012, 14019,  2014,\n",
       "          6898,  2206,  4079,  1005,  1055,  6040,  2044,  2002,  2876,\n",
       "          1005,  1056,  2031,  3348,  2007,  2014,  2021,  2101, 11323,\n",
       "          2023,  2001,  2349,  2000,  2032,  9105, 26076,  2125,  2014,\n",
       "          2767, 15595,  1012,   102,  4013,  3630,  4609,  1024,  2014,\n",
       "           102,  9178,  1024, 19431, 13737,   102,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       dtype=int32)>,\n",
       " <tf.Tensor: shape=(1, 512), dtype=int32, numpy=\n",
       " array([[  101, 11199, 10093,  3877,  1011,  1011,  2209,  1996,  2610,\n",
       "          2961,  6513,  1997,  4079,  1010,  8538,  1012, 14019,  2011,\n",
       "          4079,  1999,  1996,  2345,  2792,  1997,  2186,  1015,  1010,\n",
       "          2044,  2002,  7771,  2007,  8437,  1010,  1998,  2003,  2025,\n",
       "          2464,  2153,  1012, 18188,  2726,  2209, 19431, 13737,  1010,\n",
       "         15595,  1005,  1055,  2767,  1998,  2036,  1037,  2095,  2340,\n",
       "         11136,  1999,  4079,  1005,  1055,  2465,  1012, 14019,  2014,\n",
       "          6898,  2206,  4079,  1005,  1055,  6040,  2044,  2002,  2876,\n",
       "          1005,  1056,  2031,  3348,  2007,  2014,  2021,  2101, 11323,\n",
       "          2023,  2001,  2349,  2000,  2032,  9105, 26076,  2125,  2014,\n",
       "          2767, 15595,  1012,   102,  4013,  3630,  4609,  1024,  2014,\n",
       "           102,  9178,  1024, 15595,   102,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0]],\n",
       "       dtype=int32)>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize the text\n",
    "tokens_A = tokenizer(text_with_A, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "tokens_B = tokenizer(text_with_B, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Display tokenized output\n",
    "tokens_A['input_ids'], tokens_B['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076136aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "tokens_A = tokenizer(text_with_A, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "tokens_B = tokenizer(text_with_B, padding='max_length', truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Display tokenized output\n",
    "tokens_A['input_ids'], tokens_B['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82a3121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BERT tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the examples in the GAP dataset\n",
    "def tokenize_examples(texts, pronouns, entities):\n",
    "    tokenized_inputs = []\n",
    "    for text, pronoun, entity in zip(texts, pronouns, entities):\n",
    "        # Combine text, pronoun, and entity for coreference context\n",
    "        inputs = f\"{text} [SEP] Pronoun: {pronoun} [SEP] Entity: {entity}\"\n",
    "        tokenized_inputs.append(inputs)\n",
    "    return tokenizer(tokenized_inputs, padding=True, truncation=True, return_tensors=\"tf\")\n",
    "\n",
    "# Tokenize pronoun and candidate entity pairs\n",
    "tokenized_data_A = tokenize_examples(data['Text'], data['Pronoun'], data['A'])\n",
    "tokenized_data_B = tokenize_examples(data['Text'], data['Pronoun'], data['B'])\n",
    "\n",
    "# Encode labels\n",
    "data['A-label'] = data['A-coref'].astype(int)\n",
    "data['B-label'] = data['B-coref'].astype(int)\n",
    "\n",
    "# Split into training and test sets\n",
    "X_train_A, X_test_A, y_train_A, y_test_A = train_test_split(tokenized_data_A['input_ids'], data['A-label'], test_size=0.2, random_state=42)\n",
    "X_train_B, X_test_B, y_train_B, y_test_B = train_test_split(tokenized_data_B['input_ids'], data['B-label'], test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a4da38",
   "metadata": {},
   "source": [
    "## Step 3: Model Building and Training\n",
    "\n",
    "Now, I'll build a transformer-based model using BERT to predict which entity a pronoun refers to. This model will be trained on binary labels (0 or 1) to classify whether each pronoun points to `A` or `B` for every example in the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd437c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained BERT model for sequence classification\n",
    "model_A = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "model_B = TFAutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define optimizer and compile the model\n",
    "optimizer = Adam(learning_rate=3e-5)\n",
    "model_A.compile(optimizer=optimizer, loss=model_A.compute_loss, metrics=['accuracy'])\n",
    "model_B.compile(optimizer=optimizer, loss=model_B.compute_loss, metrics=['accuracy'])\n",
    "\n",
    "# Train the models on the tokenized data\n",
    "history_A = model_A.fit(\n",
    "    X_train_A, y_train_A,\n",
    "    validation_data=(X_test_A, y_test_A),\n",
    "    epochs=3,  # Keep epochs small initially to observe baseline performance\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "history_B = model_B.fit(\n",
    "    X_train_B, y_train_B,\n",
    "    validation_data=(X_test_B, y_test_B),\n",
    "    epochs=3,\n",
    "    batch_size=16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b1f143",
   "metadata": {},
   "source": [
    "## Step 3.1: Baseline Model Evaluation\n",
    "\n",
    "To understand the performance of our initial coreference resolution model, I will evaluate it on the test set. This will provide baseline metrics such as accuracy and F1 score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac2d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set for both models\n",
    "predictions_A = model_A.predict(X_test_A).logits\n",
    "predictions_B = model_B.predict(X_test_B).logits\n",
    "\n",
    "# Get predicted labels\n",
    "predicted_labels_A = np.argmax(predictions_A, axis=1)\n",
    "predicted_labels_B = np.argmax(predictions_B, axis=1)\n",
    "\n",
    "# Calculate accuracy and F1 score for each model\n",
    "accuracy_A = accuracy_score(y_test_A, predicted_labels_A)\n",
    "f1_A = f1_score(y_test_A, predicted_labels_A)\n",
    "\n",
    "accuracy_B = accuracy_score(y_test_B, predicted_labels_B)\n",
    "f1_B = f1_score(y_test_B, predicted_labels_B)\n",
    "\n",
    "print(f\"Model A - Test Accuracy: {accuracy_A:.2f}, F1 Score: {f1_A:.2f}\")\n",
    "print(f\"Model B - Test Accuracy: {accuracy_B:.2f}, F1 Score: {f1_B:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e515b78d",
   "metadata": {},
   "source": [
    "## Step 3.2: Gender Bias Analysis\n",
    "\n",
    "As an initial step in evaluating fairness, I'll separate examples by gender and calculate accuracy and F1 scores for each gender. This will allow me to observe if the model shows any gender bias in its predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96631a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate examples by gender\n",
    "male_indices = data[data['Pronoun'] == 'he'].index\n",
    "female_indices = data[data['Pronoun'] == 'she'].index\n",
    "\n",
    "# Model A performance by gender\n",
    "accuracy_A_male = accuracy_score(y_test_A[male_indices], predicted_labels_A[male_indices])\n",
    "accuracy_A_female = accuracy_score(y_test_A[female_indices], predicted_labels_A[female_indices])\n",
    "\n",
    "print(f\"Model A - Male Pronouns Accuracy: {accuracy_A_male:.2f}\")\n",
    "print(f\"Model A - Female Pronouns Accuracy: {accuracy_A_female:.2f}\")\n",
    "\n",
    "# Similarly, evaluate for Model B if necessary.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
