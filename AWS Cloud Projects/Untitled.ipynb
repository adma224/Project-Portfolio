{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5714cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import tarfile\n",
    "import boto3\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from dotenv import load_dotenv\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "\n",
    "import \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdebb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv('path/to/your/.env')\n",
    "\n",
    "def create_s3_bucket_name(base_string):\n",
    "    random_string = ''.join(random.choices(string.digits, k=10))\n",
    "    return f\"{base_string}-{random_string}\"\n",
    "\n",
    "def create_s3_bucket(bucket_name):\n",
    "    try:\n",
    "        s3 = boto3.client('s3', region_name=os.environ.get('AWS_DEFAULT_REGION'))\n",
    "        s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={'LocationConstraint': os.environ.get('AWS_DEFAULT_REGION')})\n",
    "        print(f\"Bucket {bucket_name} created.\")\n",
    "    except boto3.exceptions.Boto3Error as e:\n",
    "        print(f\"Failed to create bucket: {e}\")\n",
    "\n",
    "def retry_upload(local_path, bucket_name, s3_path, retries=3, delay=2):\n",
    "    s3 = boto3.client('s3', region_name=os.environ.get('AWS_DEFAULT_REGION'))\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            s3.upload_file(local_path, bucket_name, s3_path)\n",
    "            print(f\"Uploaded: {s3_path}\")\n",
    "            return\n",
    "        except ClientError as e:\n",
    "            print(f\"Retry {attempt+1}/{retries} failed for {local_path}. Error: {e}\")\n",
    "            time.sleep(delay)\n",
    "    print(f\"Failed to upload {local_path} after {retries} retries.\")\n",
    "\n",
    "def upload_directory_to_s3(bucket_name, directory_path):\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            try:\n",
    "                local_path = os.path.join(root, file)\n",
    "                s3_path = os.path.relpath(local_path, start=directory_path)\n",
    "                retry_upload(local_path, bucket_name, s3_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {local_path}. Error: {e}\")\n",
    "\n",
    "# URL of the Stanford Dogs Dataset\n",
    "url = 'http://vision.stanford.edu/aditya86/ImageNetDogs/images.tar'\n",
    "\n",
    "# Download the dataset\n",
    "try:\n",
    "    print(\"Downloading dataset...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    tar_file_path = 'stanford_dogs_dataset.tar'\n",
    "    with open(tar_file_path, 'wb') as file:\n",
    "        for chunk in response.iter_content(chunk_size=1024):\n",
    "            file.write(chunk)\n",
    "except requests.exceptions.RequestException as e:\n",
    "    print(f\"Error downloading dataset: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Decompress the tar file\n",
    "try:\n",
    "    print(\"Decompressing the dataset...\")\n",
    "    with tarfile.open(tar_file_path, 'r') as tar:\n",
    "        tar.extractall(path='stanford_dogs_dataset')\n",
    "    print('Download and extraction complete!')\n",
    "except (tarfile.TarError, IOError) as e:\n",
    "    print(f\"Error extracting tar file: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create S3 Bucket\n",
    "base_bucket_name = 'stanford-dogs-dataset'\n",
    "bucket_name = create_s3_bucket_name(base_bucket_name)\n",
    "create_s3_bucket(bucket_name)\n",
    "\n",
    "# Save the bucket name in the .env file\n",
    "with open('path/to/your/.env', 'a') as file:\n",
    "    file.write(f\"\\nMY_APP_BUCKET_NAME={bucket_name}\")\n",
    "\n",
    "# Upload to S3\n",
    "directory_path = 'stanford_dogs_dataset'  # Local directory with the dataset\n",
    "print(\"Uploading to S3...\")\n",
    "upload_directory_to_s3(bucket_name, directory_path)\n",
    "print(\"Upload complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137be40c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a678f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107c0e7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7373173a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15519a26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e443dc28",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.pytorch import PyTorch\n",
    "from sagemaker.tuner import IntegerParameter, ContinuousParameter, HyperparameterTuner\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv('path/to/your/.env')\n",
    "\n",
    "# Initialize the SageMaker session and role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "# Retrieve the bucket name from the environment variable\n",
    "bucket_name = os.environ.get('MY_APP_BUCKET_NAME')\n",
    "data_uri = f's3://{bucket_name}/stanford_dogs_dataset'\n",
    "\n",
    "# Define the SageMaker estimator\n",
    "try:\n",
    "    estimator = PyTorch(entry_point='vggnet_train.py',  # Replace with your training script\n",
    "                        role=role,\n",
    "                        framework_version='1.8.0',\n",
    "                        py_version='py3',\n",
    "                        instance_count=1,\n",
    "                        instance_type='ml.p3.2xlarge',\n",
    "                        hyperparameters={\n",
    "                            'model_type': 'vgg',\n",
    "                            'epochs': 10\n",
    "                        })\n",
    "\n",
    "    # Hyperparameter tuning\n",
    "    hyperparameter_ranges = {\n",
    "        'learning_rate': ContinuousParameter(0.001, 0.1),\n",
    "        'batch_size': IntegerParameter(32, 256),\n",
    "    }\n",
    "\n",
    "    objective_metric_name = 'validation:accuracy'\n",
    "\n",
    "    tuner = HyperparameterTuner(estimator,\n",
    "                                objective_metric_name,\n",
    "                                hyperparameter_ranges,\n",
    "                                max_jobs=20,\n",
    "                                max_parallel_jobs=3)\n",
    "\n",
    "    # Start hyperparameter tuning\n",
    "    tuner.fit({'training': data_uri})\n",
    "\n",
    "    # Deploy the best model\n",
    "    best_model = tuner.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n",
    "\n",
    "    # Optionally update the .env file with the SageMaker instance name\n",
    "    with open('path/to/your/.env', 'a') as file:\n",
    "        file.write(f\"\\nMY_APP_SAGEMAKER_INSTANCE_NAME={best_model.endpoint_name}\")\n",
    "\n",
    "except boto3.exceptions.Boto3Error as e:\n",
    "    print(f\"Error in SageMaker operation: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
